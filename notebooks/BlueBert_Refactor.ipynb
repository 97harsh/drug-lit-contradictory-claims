{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlueBert Refactor",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W11xt98unJmL",
        "colab_type": "text"
      },
      "source": [
        "# Training BlueBERT for contradiction classification\n",
        "\n",
        "This notebook uses transfer learning on BlueBert to train a binary classification model to determine if a pair of drug-treatment sentences contain any contradiction.\n",
        "\n",
        "Code based on [this](https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta) and [this](https://github.com/CoronaWhy/drug-lit-contradictory-claims/blob/master/src/contradictory_claims/data/make_dataset.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBrDRN7JHUvD",
        "colab_type": "text"
      },
      "source": [
        "### To Run:\n",
        "\n",
        "Make sure you have the working directory setup as in [this](https://github.com/CoronaWhy/drug-lit-contradictory-claims/blob/master/src/contradictory_claims/data/make_dataset.py) notebook.\n",
        "\n",
        "Install PyTorch BlueBert:\n",
        "\n",
        "Clone [BlueBert repo](https://github.com/ncbi-nlp/bluebert) into current proj_path\n",
        "\n",
        "Mac Run:\n",
        "```\n",
        "export NCBI_DIR=<directory_path_to_NCBI_BERT>transformers-cli convert --model_type bert \\\n",
        "  --tf_checkpoint $NCBI_DIR/bert_model.ckpt \\\n",
        "  --config $NCBI_DIR/bert_config.json \\\n",
        "  --pytorch_dump_output $NCBI_DIR/pytorch_model.bin\n",
        "```\n",
        "\n",
        "Windows Run:\n",
        "\n",
        "\n",
        "```\n",
        "set NCBI_DIR=<directory_path_to_NCBI_BERT>python convert_bert_original_tf_checkpoint_to_pytorch.py --tf_checkpoint_path=%NCBI_Directory%/bert_model.ckpt --bert_config_file=%NCBI_Directory%/bert_config.json --pytorch_dump_path=%NCBI_Directory%/pytorch_model.bin\n",
        "```\n",
        "\n",
        "Rename bert-config.json -> config.json\n",
        "\n",
        "Alternatively, follow the instructions [here](https://medium.com/@manasmohanty/ncbi-bluebert-ncbi-bert-using-tensorflow-weights-with-huggingface-transformers-15a7ec27fc3d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-epJZAmiTqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only necessary if running in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed1LkckeibE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%capture\n",
        "!pip install transformers\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import xml.etree.ElementTree as et \n",
        "from itertools import permutations\n",
        "\n",
        "\n",
        "from keras.utils import np_utils\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import transformers\n",
        "from transformers import AutoModel\n",
        "from transformers import AdamW, TFAutoModel, AutoTokenizer, AutoModelWithLMHead, BertTokenizer, BertModel, TFBertModel, get_linear_schedule_with_warmup\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from torchsummary import summary\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJFFZCNTGIdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")    \n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdK5jQV1gGw",
        "colab_type": "text"
      },
      "source": [
        "### Model definition and preprocessing utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXiVnzQXvBrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regular_encode(texts, tokenizer, max_len=512):\n",
        "    \"\"\"Tokenize a sentence as an np.array.\"\"\"\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        max_length=max_len,\n",
        "        truncation=True\n",
        "    )\n",
        "    return np.array(enc_di['input_ids'])\n",
        "\n",
        "def batchify(data, batch_size=50, device=torch.device(\"cpu\")):\n",
        "    tensor = torch.from_numpy(data).float()\n",
        "    x = tensor.size(0)\n",
        "    y = tensor.size(1)\n",
        "    batches = x // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    tensor = tensor.narrow(0, 0, batches * batch_size)\n",
        "    # Evenly divide the data across the batch_size batches.\n",
        "    tensor = tensor.view(-1, batch_size, y).contiguous()\n",
        "    return tensor.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3zu0H_UVCjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContraDataset(Dataset):\n",
        "  \"\"\"Dataset loader.\"\"\"\n",
        "  \n",
        "  def __init__(self, claims, labels, tokenizer, max_len=512):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.claims = torch.tensor(self.regular_encode(claims, max_len=max_len))\n",
        "    self.att_mask = torch.zeros(self.claims.size())\n",
        "    self.att_mask = torch.where(self.claims <= self.att_mask,  self.att_mask, torch.ones(self.claims.size()))\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    assert index < len(self.labels)\n",
        "    return (self.claims[index], self.att_mask[index], torch.tensor(self.labels[index]))\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.labels.shape[0]\n",
        "\n",
        "  def regular_encode(self, texts, max_len=512):\n",
        "    \"\"\"Tokenize a batch of sentence as an np.array.\"\"\"\n",
        "    enc_di = self.tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        max_length=max_len,\n",
        "        truncation=True\n",
        "    )\n",
        "    return np.array(enc_di['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR5zkb5zd0sS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TorchContraNet(nn.Module):\n",
        "  \"\"\"Our transfer learning trained model.\"\"\"\n",
        "\n",
        "  def __init__(self, transformer):\n",
        "    \"\"\"Define and initialize the layers of the model.\"\"\"\n",
        "    super(TorchContraNet, self).__init__()\n",
        "    self.transformer = transformer\n",
        "    self.linear = nn.Linear(768, 3)\n",
        "    self.out = nn.Softmax(dim=0)\n",
        "    \n",
        "  def forward(self, claim, mask, label=None):\n",
        "    \"\"\"Run the model on inputs.\"\"\"\n",
        "    hidden_states, enc_attn_mask = self.transformer(claim, \n",
        "                                                    token_type_ids=None,\n",
        "                                                    attention_mask=mask)\n",
        "    unnormalized_labels = self.linear(hidden_states[:, 0, :])\n",
        "    y = self.out(unnormalized_labels)\n",
        "    return y\n",
        "  \n",
        "  def train(self):\n",
        "    # Prepare transformer for training\n",
        "    self.transformer.train()\n",
        "\n",
        "  def eval(self):\n",
        "    # Freeze transformer weights\n",
        "    self.transformer.eval()\n",
        "\n",
        "#TODO implement TF version using BERT as a Service"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT8MeLSznArN",
        "colab_type": "text"
      },
      "source": [
        "### Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWFdPhg0kfVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Contains project hyperparameters and paths\n",
        "\n",
        "# Root path\n",
        "proj_path = Path('/content/gdrive/My Drive/Colab Notebooks/BlueBERT/')\n",
        "\n",
        "# Pretrained models\n",
        "models_path = proj_path / 'models'\n",
        "bluebert_path = models_path / 'BlueBERT'\n",
        "\n",
        "# Top level dataset paths\n",
        "inputs_path = proj_path / 'Input'\n",
        "multinli_path = inputs_path / 'multinli'\n",
        "mednli_path = inputs_path / 'mednli'\n",
        "mancon_path = inputs_path / 'manconcorpus-sent-pairs'\n",
        "drug_path = inputs_path / 'drugnames'\n",
        "virus_path = inputs_path / 'virus-words'\n",
        "\n",
        "# MultiNLI\n",
        "multinli_train_path = multinli_path / 'multinli_1.0_train.txt'\n",
        "multinli_test_path = multinli_path / 'multinli_1.0_dev_matched.txt'\n",
        "MULTINLI_DATA_PER_CATEGORY = 1000\n",
        "\n",
        "# Mancon\n",
        "mancon_sent_pairs = mancon_path / 'manconcorpus_sent_pairs_200516.tsv'\n",
        "MANCON_DATA_PER_CATEGORY = 1000\n",
        "\n",
        "# MedNLI\n",
        "#TODO add in the rest of the MedNLI ingestion and preprocessing\n",
        "USING_MEDNLI = False # Does the user have access/did the training for mednli?\n",
        "if USING_MEDNLI:\n",
        "  mednli_train_path = mednli_path / 'mli_train_v1.jsonl'\n",
        "  mednli_dev_path = mednli_path / 'mli_dev_v1.jsonl'\n",
        "  mednli_test_path = mednli_path / 'mli_test_v1.jsonl'\n",
        "  MEDNLI_DATA_PER_CATEGORY = 1000\n",
        "\n",
        "# Additional tokenizer dataset path\n",
        "drug_names_path = drug_path / 'DrugNames.txt'\n",
        "virus_names_path = virus_path / 'virus_words.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpD8-O-vdzlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pretrained BlueBert\n",
        "PRETRAINED_PATH = bluebert_path\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(str(PRETRAINED_PATH))\n",
        "transformer = BertModel.from_pretrained(str(PRETRAINED_PATH), num_labels=3, output_attentions=False, output_hidden_states=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGlMpYEjjQzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in MultiNLI\n",
        "multinli_train_data = pd.read_csv(multinli_train_path, sep='\\t', error_bad_lines=False)\n",
        "multinli_test_data = pd.read_csv(multinli_test_path, sep='\\t', error_bad_lines=False)\n",
        "\n",
        "multinli_train_data['gold_label'] = [2 if l=='contradiction' else 1 if l=='entailment' else 0 for l in multinli_train_data['gold_label']]\n",
        "multinli_test_data['gold_label'] = [2 if l=='contradiction' else 1 if l=='entailment' else 0 for l in multinli_test_data['gold_label']]\n",
        "\n",
        "# Remove rows with NaN in either sentence1 or sentence2 since NaN != NaN\n",
        "multinli_train_data = multinli_train_data[multinli_train_data['sentence1'] == multinli_train_data['sentence1']][multinli_train_data['sentence2'] == multinli_train_data['sentence2']]\n",
        "multinli_test_data = multinli_test_data[multinli_test_data['sentence1'] == multinli_test_data['sentence1']][multinli_test_data['sentence2'] == multinli_test_data['sentence2']]\n",
        "\n",
        "#TODO add range checks\n",
        "balanced_multinli_train_data = multinli_train_data[multinli_train_data['gold_label']==2].head(MULTINLI_DATA_PER_CATEGORY).reset_index(drop=True) \n",
        "balanced_multinli_train_data = balanced_multinli_train_data.append(multinli_train_data[multinli_train_data['gold_label']==1].head(MULTINLI_DATA_PER_CATEGORY)).reset_index(drop=True)\n",
        "balanced_multinli_train_data = balanced_multinli_train_data.append(multinli_train_data[multinli_train_data['gold_label']==0].head(MULTINLI_DATA_PER_CATEGORY)).reset_index(drop=True)\n",
        "\n",
        "# Make data into the form that BERT expects\n",
        "multinli_x_train = '[CLS]' + balanced_multinli_train_data.sentence1 + '[SEP]' + balanced_multinli_train_data.sentence2\n",
        "multinli_x_test = '[CLS]' + multinli_test_data.sentence1 + '[SEP]' + multinli_test_data.sentence2\n",
        "multinli_y_train = np_utils.to_categorical(balanced_multinli_train_data['gold_label'], dtype='int')\n",
        "multinli_y_test = np_utils.to_categorical(multinli_test_data['gold_label'], dtype='int')\n",
        "\n",
        "# Package data into a DataLoader\n",
        "multinli_x_train_dataset = ContraDataset(multinli_x_train.to_list(), multinli_y_train, tokenizer, max_len=64)\n",
        "multinli_x_train_sampler = RandomSampler(multinli_x_train_dataset)\n",
        "multinli_x_train_dataloader = DataLoader(multinli_x_train_dataset, sampler=multinli_x_train_sampler, batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP9wS7Sqkf3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ingest Mancon data\n",
        "raw_mancon_data = pd.read_csv(mancon_sent_pairs, sep ='\\t')\n",
        "raw_mancon_data['label'] = [2 if l=='contradiction' else 1 if l=='entailment' else 0 for l in raw_mancon_data['label']]\n",
        "raw_mancon_data['label'] = raw_mancon_data['label'].astype('float')\n",
        "\n",
        "#TODO add range checks\n",
        "balanced_mancon_data = raw_mancon_data[raw_mancon_data['label']==2].head(MANCON_DATA_PER_CATEGORY).reset_index(drop=True)\n",
        "balanced_mancon_data = balanced_mancon_data.append(raw_mancon_data[raw_mancon_data['label']==1].head(MANCON_DATA_PER_CATEGORY)).reset_index(drop=True)\n",
        "balanced_mancon_data = balanced_mancon_data.append(raw_mancon_data[raw_mancon_data['label']==0].head(MANCON_DATA_PER_CATEGORY)).reset_index(drop=True)\n",
        "\n",
        "mancon_x_train, mancon_x_test, mancon_y_train, mancon_y_test = train_test_split('[CLS]' + balanced_mancon_data['text_a'] + '[SEP]' + balanced_mancon_data['text_b'], balanced_mancon_data['label'], test_size=0.2)\n",
        "mancon_y_train = np_utils.to_categorical(mancon_y_train)\n",
        "mancon_y_test = np_utils.to_categorical(mancon_y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R8XDq8fdN_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in drug to help augment tokenizer\n",
        "drug_names = pd.read_csv(drug_names_path,header=None)\n",
        "drug_names = list(drug_names[0])\n",
        "\n",
        "# Only want the drugs mentioned in our datasets\n",
        "#TODO add multinli and mednli mentions to filter drug names too\n",
        "mancon_text = ' '.join(list(set(balanced_mancon_data.text_a)))\n",
        "drug_names = [drug for drug in drug_names if drug in mancon_text]\n",
        "\n",
        "\n",
        "# Read in virus names to help augment tokenizer\n",
        "virus_names = pd.read_csv(virus_names_path, header=None)\n",
        "virus_names = list(virus_names[0])\n",
        "\n",
        "# Add drug and virus names to existing tokenizer\n",
        "tokenizer.add_tokens(drug_names + virus_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doAYhvbfyj-N",
        "colab_type": "text"
      },
      "source": [
        "### Train Utilities\n",
        "\n",
        "Functions adapted from [here](https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JevyjtDwu3sW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "  # Get index of largest softmax prediction\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = np.argmax(labels, axis=1).flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "  '''\n",
        "  Takes a time in seconds and returns a string hh:mm:ss\n",
        "  '''\n",
        "  # Round to the nearest second.\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "  # Format as hh:mm:ss\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL7UcDq1sKIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,\n",
        "          dataloader,\n",
        "          device,\n",
        "          criterion=torch.nn.MSELoss(reduction='sum'),\n",
        "          optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8),\n",
        "          epochs=3,\n",
        "          seed=42):\n",
        "  \n",
        "  # Set the seed value all over the place to make this reproducible.\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  loss_values = []\n",
        "  \n",
        "  # Initialize scheduler\n",
        "  total_steps = len(dataloader) * epochs\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                              num_warmup_steps = 0,\n",
        "                                              num_training_steps = total_steps)\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()    \n",
        "    \n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Step through dataloader output\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      # Progress update every 40 batches.\n",
        "      if step % 40 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader), elapsed))\n",
        "\n",
        "      claim = batch[0].to(device)\n",
        "      mask = batch[1].to(device)\n",
        "      label = batch[2].to(device).float()\n",
        "\n",
        "      model.zero_grad()\n",
        "      \n",
        "      y = model(claim, mask)\n",
        "      loss = criterion(y, label)\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      \n",
        "    avg_train_loss = total_loss / len(dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)    \n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "  return loss_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaWt-iKpzdOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model,\n",
        "             dataloader,\n",
        "             device):\n",
        "  print(\"\")\n",
        "  print(\"Running Validation...\")    \n",
        "  t0 = time.time()\n",
        "\n",
        "  # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
        "  model.eval()\n",
        "  \n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  \n",
        "  # Evaluate data for one epoch\n",
        "  for batch in dataloader:\n",
        "      \n",
        "    claim = batch[0].to(device)\n",
        "    mask = batch[1].to(device)\n",
        "    label = batch[2].to(device).float()\n",
        "\n",
        "    with torch.no_grad():                    \n",
        "      pred_labels = model(claim, mask)\n",
        "    \n",
        "    pred_labels = pred_labels.detach().cpu().numpy()\n",
        "    label = label.to('cpu').numpy()\n",
        "  \n",
        "    # Calculate the accuracy\n",
        "    tmp_eval_accuracy = flat_accuracy(pred_labels, label)\n",
        "    \n",
        "    # Accumulate the total accuracy.\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "  print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olFyKehZuH9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model\n",
        "model = TorchContraNet(transformer)\n",
        "model.train()\n",
        "model.to(device)\n",
        "\n",
        "# Train model\n",
        "losses = train(model, multinli_x_train_dataloader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QOuYYGdU-ZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validate(model, multinli_x_train_dataloader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xksnJI5Cs3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}